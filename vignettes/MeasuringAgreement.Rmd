---
title: "Measues of Agreement"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MeasuringAgreement}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(kableExtra)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(CPTtools)
```

This vignette will use measures of agreement, in particular, the Fleis-Cohen kappa, and the Goodman-Kruskal lambda to explore the proposed adequacy of a cognitively diagnostic assessment.

# Assessment Design

This assessment is based on an example found in [@Mislevy1995].  It is a test of language arts in which there are four constructs being measured:  _Reading_, _Writing_, _Speaking_ and _Listening_.  Each variable can tale on the possible values `Advanced`, `Intermediate` or `Novice`.

There are four kinds of tasks:

Reading
: Subjects read a short segment of text and then answer a selected response question.

Writing
: An integrated Reading/Writing task where the subject provides a short written response based on an reading passage.

Listening
: Subject listens to a prompt followed by a mulitple choice question where the options are also spoken (as are the instructions).

Speaking
: Subject is requested to respond verbally (response is recorded) after listening to an audio stimulus.  The instructions are written.

## Form Design

There are 5 Reading, 5 Listening, 3 Writing and 3 Speaking questions on a form of the test.  Therefore, the Q Matrix looks like:

```{r Q-matrix for language test, echo=FALSE}
qmat <- matrix(c(rep(c(1,0,0,0),5),
                 rep(c(1,1,0,0),3),
                 rep(c(0,0,0,1),5),
                 rep(c(1,0,1,1),3)),
               16,4,
               byrow=TRUE)
colnames(qmat)<-c("Reading","Writing","Speaking","Listening")
rownames(qmat)<-paste0(c(rep("R",5),rep("W",3),
                         rep("L",5),rep("S",3)),1:16)
kbl(qmat,caption="Q-matrix for 16 item test") |>
  kable_classic(full_width=FALSE)
```

## Simulation Experiment

Assuming that the parameters of the 5 item models are known, it is straightforward to simulate data from the assessment.  This kind of simulation provides information about the adequacy of the data collection for classifying the students.

The simulation procedure is as follows:

1. Generate random proficiency profiles using the proficiency model.

2. Generate random item responses using the item (evidence) models.

3. Score the assessment.  There are two variations:
  
  a. _Modal (MAP) Scores_:  The score assigns a category to each of the four constructs.  (These are the columns named "mode.Reading" and similar.)
  
  b. _Expected (Probability) Scores_:  The score assigns a probability of high, medium or low to each individual.  (These are the columns named "Reading.Novice", "Reading.Intermediate", and "Reading.Advanced".)
  
The simulation study itself can be found in `vingette("SimulationStudies",package="RNetica")`.

The results are saved in this package in the data set `language16`.

```{r}
data(language16)
```

## Building the Confusion Matrix

The simulation data has columns containing the "true" value for the four proficiencies---"Reading", "Writing", "Speaking", and "Listening"---and for values for the estimates---"mode.Reading", "mode.Writing", &c.  The cross-tabulation of a true value and its corresponding estimate is known as a confusion matrix.  This can be built in R using the `table` function.[^2]

[^1]: It is in the `Analysis > Descriptives > Cross Tabluations ...` function in SPSS.

```{r Reading.cm}
Reading.cm <- table(language16[,c("Reading","mode.Reading")])
```
```{r Reading.tab,echo=FALSE}
kbl(Reading.cm, caption="Reading confusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

```{r Writing.cm}
Writing.cm <- table(language16[,c("Writing","mode.Writing")])
```
```{r Writing.tab,echo=FALSE}
kbl(Writing.cm, caption="Writing confusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

```{r Speaking.cm}
Speaking.cm <- table(language16[,c("Speaking","mode.Speaking")])
```
```{r Speaking.tab,echo=FALSE}
kbl(Speaking.cm, caption="Speaking Confusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

```{r Listening.cm}
Listening.cm <- table(language16[,c("Listening","mode.Listening")])
```
```{r Listening.tab,echo=FALSE}
kbl(Listening.cm, caption="Listening Confusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

### The expected Confusion Matrix

The model used in this simulation is a Bayesian network.  Like many
kinds of classication models its output is the probability that the
subject is in each category.  So the columns `Reading.Novice`,
`Reading.Intermediate`, and `Reading.Advanced` represent the
probability based on the observed evidence that the simulee's reading
ability is in those threee categories.  The `mode.Reading`, which we
used as the score before, just tells which category has the highest
probability.   The table below shows just the reading scores for the
first 5 simulees.

```{r Reading5}
kbl(language16[1:5,c("Reading","Reading.Novice",
                     "Reading.Intermediate","Reading.Advanced",
					 "mode.Reading")],
     caption="Reading data from first five simulees.",
     digits=3) |>
  kable_classic()
```

Look at the first row of the data.  The first simulee was randomly
assigned a reading ability of `Intermediate` and then the data were
generated based on this estimate.  The classification of the first
simulee based on the 16 item form is a 56.5\% chance of `Novice` and a
43.5\% chance of `Intermediate`.  So, the "modal" (highest
probability) score is `Novice`.

When building the confusion matrix, each observation add a count of
one to a cell based on the true and estimated value.  So the first
simulee contributes 1 towards the cell in the second row and first
column in the table.  In building the expected table, this value is
split across the row according to the probabilities, so the first
simulee adds .565 to the first cell in the second row, .435 to the
second cell and nothing to the third cell.  

The table below shows the calculations for the first five simulees.
The first row is the sum of the `Reading.Novice`,
`Reading.Intermediate` and `Reading.Advanced` variable for simulees
whose true Reading value is `Novice`, the second for true
`Intermediate`, and third row for true `Advanced.`


```{r}
reading5 <- expTable(language16[1:5,],"Reading","Reading")
```
```{r Reading.extab,echo=FALSE}
kbl(reading5, digits=3,
    caption="Expected reading confusion matrix using 5 items.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

Based on this definition of the expected confusion matrix, here are
the values for the four modalities.

```{r Reading.em}
Reading.em <- expTable(language16,"Reading","Reading")
```
```{r Reading.etab,echo=FALSE}
kbl(Reading.em, digits=3,
    caption="Reading expected onfusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

```{r Writing.em}
Writing.em <- expTable(language16,"Writing","Writing")
```
```{r Writing.etab,echo=FALSE}
kbl(Writing.em, digits=3,
    caption="Writing expected onfusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

```{r Speaking.em}
Speaking.em <- expTable(language16,"Speaking","Speaking")
```
```{r Speaking.etab,echo=FALSE}
kbl(Speaking.em, digits=3,
    caption="Speaking expected onfusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

```{r Listening.em}
Listening.em <- expTable(language16,"Listening","Listening")
```
```{r Listening.etab,echo=FALSE}
kbl(Listening.em, digits=3,
    caption="Listening expected onfusion matrix for 16 item test.") |>
  kable_classic(full_width=FALSE) |>
  add_header_above(c(" ","Estimated"=3)) |>
  pack_rows(index=c("Simulated"=3))
```

# Measuring agreement.

## Unweighted Measures of Agreement

### Raw Agreement

If we look at the diagonal of one of these confusion matrixes, this is the number of cases that exactly agree between the simulated truth and the estimate.  The proportion is the agreement rate.

```{r}
sum(diag(Reading.cm))/sum(Reading.cm)
```
Obviously the closer to 1, the better.  

### Goodman-Kruskal Lambda

There is a problem with the agreement rate:  depending on the proportions of cases in the population, it could be very easy to get a good agreement rate.  Suppose the question is whether or not a randomly chosen person has traveled beyond the Earth's atmosphere.  As there are only a few dozen astronauts and cosmonauts, just guessing that everybody has not been to space is likely to get 100\% accuracy. 

@Goodman1954 suggested a correction.  Pick the row which has the biggest proportion.  For the Reading data, this is `Intermediate`, there are 480 intermediate readers in the sample, so simply classifying everybody as an intermediate would correctly classify 48\% of the sample.  

Let $p_{ij}$ be the proportion of cases where the "true" category was $i$ and the estimated category was $j$.  Let $p_{i+} = \sum_{j} p_{ij}$ be the row sums and $\sum_{i} p_{ii}$ is the agreement.
Lambda discounts the agreement by the biggest of the row sums.

$$ \lambda = \frac{\sum p_{ii} - \max_{i} p_{i+}}{1 - \max_{i} p_{i+}}$$

Here is the value for the Reading data.

```{r}
gkLambda(Reading.cm)
```
This is pretty good.  There is a 67.5\% improvement in classification from the one-size-fits-all model of classifying everybody at `Intermediate`.

### Fleis-Cohen Kappa

Jacob Cohen proposed a different way to correct for distribution of the categories in the population; one that does not regard one of the two classifications as "truth".  If the first rater is randomly classifying subjects with the probabilities $p_{i+}$ and the second with probabilities $p_{+j}$ (these are the row and column sums respectively).  Then the probability of a random agreement is $\sum p_{i+}p_{+j}$.  He called this measure kappa [@Fleiss2003].

~~~~28
Kappa
```{r}
fcKappa(Reading.cm)
```


## Weighted Kappas and Lambdas

